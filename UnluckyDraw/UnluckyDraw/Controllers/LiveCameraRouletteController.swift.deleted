//
//  LiveCameraRouletteController.swift
//  UnluckyDraw
//
//  Created on 2025-06-16
//

import Foundation
import AVFoundation
import Vision
import UIKit
import Combine

class LiveCameraRouletteController: NSObject, ObservableObject {
    // MARK: - Published Properties
    @Published var detectedFaces: [DetectedFace] = []
    @Published var currentHighlightedIndex: Int = 0
    @Published var isSpinning = false
    @Published var winner: DetectedFace?
    @Published var cameraPermissionStatus: CameraPermissionStatus = .unknown
    @Published var cameraPosition: AVCaptureDevice.Position = .back
    
    enum CameraPermissionStatus {
        case unknown
        case granted
        case denied
        case restricted
    }
    
    // MARK: - Camera Properties
    private var captureSession: AVCaptureSession?
    private var videoOutput: AVCaptureVideoDataOutput?
    private var _previewLayer: AVCaptureVideoPreviewLayer?
    private var currentCamera: AVCaptureDevice?
    
    // MARK: - Face Capture Properties
    @Published var capturedWinnerImage: UIImage?
    private var latestCameraFrame: CVPixelBuffer?
    
    // MARK: - Face Detection Properties
    private var faceDetectionRequest: VNDetectFaceRectanglesRequest?
    private let faceDetectionQueue = DispatchQueue(label: "faceDetection", qos: .userInitiated)
    
    // MARK: - Roulette Properties
    private var rouletteTimer: Timer?
    private var spinStartTime: Date?
    private let totalSpinDuration: Double = 4.0
    private let initialSpeed: Double = 0.05
    private let finalSpeed: Double = 0.3
    
    // MARK: - Performance Optimization
    private var lastProcessTime: Date = Date()
    private let processingInterval: TimeInterval = 0.1 // 10 FPS for face detection
    
    override init() {
        super.init()
        setupFaceDetection()
    }
    
    // MARK: - Public Interface
    var previewLayer: AVCaptureVideoPreviewLayer? {
        return self._previewLayer
    }
    
    func startCamera() {
        print("ğŸ“· Starting live camera...")
        checkCameraPermission { [weak self] granted in
            DispatchQueue.main.async {
                if granted {
                    print("âœ… Camera permission granted")
                    self?.setupCamera()
                    // setupCamera ë‚´ë¶€ì—ì„œ ì„¸ì…˜ì„ ì‹œì‘í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” í˜¸ì¶œí•˜ì§€ ì•ŠìŒ
                } else {
                    print("âŒ Camera permission denied")
                    // ê¶Œí•œì´ ê±°ë¶€ëœ ê²½ìš° ì‚¬ìš©ìì—ê²Œ ì•Œë¦¼
                }
            }
        }
    }
    
    func stopCamera() {
        print("ğŸ“· Stopping live camera...")
        captureSession?.stopRunning()
        stopRoulette()
    }
    
    func toggleCamera() {
        print("ğŸ“· Toggling camera from \(cameraPosition) to \(cameraPosition == .back ? "front" : "back")")
        
        // ë£°ë ›ì´ ëŒì•„ê°€ëŠ” ì¤‘ì´ë©´ ì¤‘ì§€
        if isSpinning {
            stopRoulette()
        }
        
        // ì–¼êµ´ ê°ì§€ ì´ˆê¸°í™”
        detectedFaces.removeAll()
        winner = nil
        
        // ì¹´ë©”ë¼ ìœ„ì¹˜ ë³€ê²½
        cameraPosition = cameraPosition == .back ? .front : .back
        
        // ì•ˆì „í•œ ì¹´ë©”ë¼ ì¬ì„¤ì •
        DispatchQueue.global(qos: .userInitiated).async {
            self.setupCamera()
        }
    }
    
    func startRoulette() {
        guard detectedFaces.count >= 2 else {
            print("âŒ Cannot start roulette: need at least 2 faces")
            return
        }
        
        print("ğŸ° Starting live camera roulette with \(detectedFaces.count) faces")
        
        isSpinning = true
        winner = nil
        currentHighlightedIndex = 0
        spinStartTime = Date()
        
        // ì‹œì‘ ì‚¬ìš´ë“œ
        SoundManager.shared.playStartSound()
        
        // ë£°ë › íƒ€ì´ë¨¸ ì‹œì‘
        startRouletteAnimation()
        
        // ìë™ ì¢…ë£Œ íƒ€ì´ë¨¸
        DispatchQueue.main.asyncAfter(deadline: .now() + totalSpinDuration) { [weak self] in
            self?.stopRoulette()
        }
    }
    
    func stopRoulette() {
        print("ğŸ° Stopping roulette")
        
        isSpinning = false
        rouletteTimer?.invalidate()
        rouletteTimer = nil
        
        // ë‹¹ì²¨ì ê²°ì •
        if !detectedFaces.isEmpty {
            var winnerFace = detectedFaces[currentHighlightedIndex]
            winnerFace.isWinner = true
            self.winner = winnerFace
            
            // ë‹¹ì²¨ ì‚¬ìš´ë“œ
            SoundManager.shared.playWinSound()
            
            // í–…í‹± í”¼ë“œë°±
            let notificationFeedback = UINotificationFeedbackGenerator()
            notificationFeedback.notificationOccurred(.success)
            
            print("ğŸ† Winner selected: Face \(currentHighlightedIndex + 1)")
            
            // ë‹¹ì²¨ì ì–¼êµ´ ì¦‰ì‹œ ìº¡ì²˜
            self.captureWinnerFace()
            
            // ë‹¹ì²¨ì ì„ ì • í›„ ì–¼êµ´ ì¶”ì  ì¤‘ë‹¨ (ê¹¨ë—í•œ UIë¥¼ ìœ„í•´)
            DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
                // ë‹¹ì²¨ìë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì–¼êµ´ ìˆ¨ê¹€
                self.detectedFaces = [winnerFace]
            }
        }
    }
    
    func reset() {
        print("ğŸ”„ Resetting roulette")
        stopRoulette()
        winner = nil
        capturedWinnerImage = nil
        currentHighlightedIndex = 0
    }
    
    // MARK: - Private Methods
    
    private func checkCameraPermission(completion: @escaping (Bool) -> Void) {
        let status = AVCaptureDevice.authorizationStatus(for: .video)
        print("ğŸ“· Current camera permission status: \(status.rawValue)")
        
        switch status {
        case .authorized:
            DispatchQueue.main.async {
                self.cameraPermissionStatus = .granted
            }
            completion(true)
        case .notDetermined:
            DispatchQueue.main.async {
                self.cameraPermissionStatus = .unknown
            }
            AVCaptureDevice.requestAccess(for: .video) { [weak self] granted in
                DispatchQueue.main.async {
                    self?.cameraPermissionStatus = granted ? .granted : .denied
                    completion(granted)
                }
            }
        case .denied:
            DispatchQueue.main.async {
                self.cameraPermissionStatus = .denied
            }
            completion(false)
        case .restricted:
            DispatchQueue.main.async {
                self.cameraPermissionStatus = .restricted
            }
            completion(false)
        @unknown default:
            DispatchQueue.main.async {
                self.cameraPermissionStatus = .denied
            }
            completion(false)
        }
    }
    
    private func setupCamera() {
        // ê¸°ì¡´ ì„¸ì…˜ ì•ˆì „í•˜ê²Œ ì •ë¦¬
        if let existingSession = captureSession {
            existingSession.stopRunning()
            print("âš™ï¸ Stopped existing session")
        }
        
        // ìƒˆ ìº¡ì²˜ ì„¸ì…˜ ìƒì„±
        let newSession = AVCaptureSession()
        
        // ì„¸ì…˜ êµ¬ì„± ì‹œì‘
        newSession.beginConfiguration()
        
        // ê¸°ì¡´ ì…ë ¥/ì¶œë ¥ ì œê±°
        for input in newSession.inputs {
            newSession.removeInput(input)
        }
        for output in newSession.outputs {
            newSession.removeOutput(output)
        }
        
        // ì„¸ì…˜ í’ˆì§ˆ ì„¤ì • (ì„±ëŠ¥ê³¼ í’ˆì§ˆì˜ ê· í˜•)
        if newSession.canSetSessionPreset(.high) {
            newSession.sessionPreset = .high
        } else if newSession.canSetSessionPreset(.medium) {
            newSession.sessionPreset = .medium
        }
        
        guard let camera = getCamera(for: cameraPosition) else {
            print("âŒ Cannot find camera for position: \(cameraPosition)")
            newSession.commitConfiguration()
            return
        }
        
        do {
            // ì¹´ë©”ë¼ ì…ë ¥ ì„¤ì •
            let cameraInput = try AVCaptureDeviceInput(device: camera)
            
            if newSession.canAddInput(cameraInput) {
                newSession.addInput(cameraInput)
                currentCamera = camera
                print("âœ… Camera input added: \(camera.localizedName)")
            } else {
                print("âŒ Cannot add camera input")
                newSession.commitConfiguration()
                return
            }
            
            // ë¹„ë””ì˜¤ ì¶œë ¥ ì„¤ì •
            let videoOutput = AVCaptureVideoDataOutput()
            videoOutput.setSampleBufferDelegate(self, queue: faceDetectionQueue)
            
            // í”½ì…€ í¬ë§· ì„¤ì • (Face Detectionì— ìµœì í™”)
            videoOutput.videoSettings = [
                kCVPixelBufferPixelFormatTypeKey as String: kCVPixelFormatType_32BGRA
            ]
            
            // í”„ë ˆì„ ë“œë¡­ ë°©ì§€ (ì„±ëŠ¥ ìµœì í™”)
            videoOutput.alwaysDiscardsLateVideoFrames = true
            
            if newSession.canAddOutput(videoOutput) {
                newSession.addOutput(videoOutput)
                self.videoOutput = videoOutput
                print("âœ… Video output configured")
            } else {
                print("âŒ Cannot add video output")
                newSession.commitConfiguration()
                return
            }
            
            // ì„¸ì…˜ êµ¬ì„± ì™„ë£Œ
            newSession.commitConfiguration()
            
            // í”„ë¦¬ë·° ë ˆì´ì–´ ì„¤ì •
            let newPreviewLayer = AVCaptureVideoPreviewLayer(session: newSession)
            newPreviewLayer.videoGravity = .resizeAspectFill
            
            DispatchQueue.main.async {
                self.captureSession = newSession
                self._previewLayer = newPreviewLayer
                
                // ì„¸ì…˜ ì‹œì‘
                DispatchQueue.global(qos: .userInitiated).async {
                    newSession.startRunning()
                    DispatchQueue.main.async {
                        print("âœ… Camera capture session started with \(self.cameraPosition) camera")
                    }
                }
            }
            
            print("âœ… Camera setup completed")
            
        } catch {
            print("âŒ Camera setup failed: \(error.localizedDescription)")
            newSession.commitConfiguration()
        }
    }
    
    private func getCamera(for position: AVCaptureDevice.Position) -> AVCaptureDevice? {
        if #available(iOS 10.0, *) {
            return AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: position)
        } else {
            return AVCaptureDevice.devices(for: .video)
                .first { $0.position == position }
        }
    }
    
    private func startCaptureSession() {
        guard let session = captureSession else { return }
        
        DispatchQueue.global(qos: .userInitiated).async {
            session.startRunning()
            DispatchQueue.main.async {
                print("âœ… Camera capture session started")
            }
        }
    }
    
    private func setupFaceDetection() {
        faceDetectionRequest = VNDetectFaceRectanglesRequest { [weak self] request, error in
            self?.processFaceDetectionResults(request: request, error: error)
        }
        
        // ìµœê³  ì„±ëŠ¥ ì„¤ì •
        faceDetectionRequest?.revision = VNDetectFaceRectanglesRequestRevision3
        
        if #available(iOS 14.0, *) {
            faceDetectionRequest?.usesCPUOnly = false // GPU ê°€ì†
        }
        
        print("ğŸ¤– Live face detection configured")
    }
    
    private func processFaceDetectionResults(request: VNRequest, error: Error?) {
        // ë‹¹ì²¨ìê°€ ìˆìœ¼ë©´ ìƒˆë¡œìš´ ì–¼êµ´ ê°ì§€ ì¤‘ë‹¨
        guard winner == nil else { return }
        
        guard error == nil,
              let results = request.results as? [VNFaceObservation],
              !results.isEmpty else {
            DispatchQueue.main.async {
                if self.winner == nil {
                    self.detectedFaces = []
                }
            }
            return
        }
        
        // ê³ í’ˆì§ˆ ì–¼êµ´ë§Œ í•„í„°ë§
        let validFaces = results.compactMap { observation -> DetectedFace? in
            let boundingBox = observation.boundingBox
            let confidence = observation.confidence
            
            // ì‹ ë¢°ë„ ë° í¬ê¸° ê²€ì‚¬
            guard confidence > 0.5,
                  boundingBox.width * boundingBox.height > 0.015,
                  boundingBox.width / boundingBox.height > 0.5,
                  boundingBox.width / boundingBox.height < 2.0 else {
                return nil
            }
            
            return DetectedFace(
                boundingBox: boundingBox,
                confidence: confidence
            )
        }
        
        DispatchQueue.main.async {
            // ë‹¹ì²¨ìê°€ ì—†ì„ ë•Œë§Œ ì–¼êµ´ ì—…ë°ì´íŠ¸
            if self.winner == nil {
                self.detectedFaces = validFaces
                
                // ë£°ë › ì¤‘ì´ ì•„ë‹ ë•Œë§Œ ì¸ë±ìŠ¤ ë¦¬ì…‹
                if !self.isSpinning {
                    self.currentHighlightedIndex = 0
                }
                
                // ì–¼êµ´ì´ ì‚¬ë¼ì§€ë©´ ë£°ë › ì¤‘ì§€
                if validFaces.isEmpty && self.isSpinning {
                    self.stopRoulette()
                }
            }
        }
    }
    
    private func startRouletteAnimation() {
        let currentSpeed = calculateCurrentSpeed()
        
        rouletteTimer = Timer.scheduledTimer(withTimeInterval: currentSpeed, repeats: false) { [weak self] _ in
            self?.updateRouletteHighlight()
        }
    }
    
    private func updateRouletteHighlight() {
        guard !detectedFaces.isEmpty, isSpinning else { return }
        
        // ë‹¤ìŒ ì–¼êµ´ë¡œ ì´ë™
        currentHighlightedIndex = (currentHighlightedIndex + 1) % detectedFaces.count
        
        // ì‚¬ìš´ë“œ ë° í–…í‹±
        SoundManager.shared.playSpinSound()
        let impactFeedback = UIImpactFeedbackGenerator(style: .light)
        impactFeedback.impactOccurred()
        
        // ê³„ì† ìŠ¤í”¼ë‹ ì¤‘ì´ë©´ ë‹¤ìŒ íƒ€ì´ë¨¸ ì„¤ì •
        if isSpinning {
            startRouletteAnimation()
        }
    }
    
    private func calculateCurrentSpeed() -> Double {
        guard let startTime = spinStartTime else { return initialSpeed }
        
        let elapsedTime = Date().timeIntervalSince(startTime)
        let progress = min(elapsedTime / totalSpinDuration, 1.0)
        
        // Ease-out íš¨ê³¼ë¡œ ì ì§„ì  ê°ì†
        let easeOutProgress = 1 - pow(1 - progress, 3)
        return initialSpeed + (finalSpeed - initialSpeed) * easeOutProgress
    }
    
    // MARK: - Face Capture Methods
    
    func captureWinnerFace() {
        guard let winner = self.winner,
              let pixelBuffer = latestCameraFrame else {
            print("âŒ Cannot capture winner face: missing data")
            return
        }
        
        // ì¹´ë©”ë¼ í”„ë ˆì„ì—ì„œ ì–¼êµ´ ì˜ì—­ ì¶”ì¶œ
        DispatchQueue.global(qos: .userInitiated).async {
            if let faceImage = self.extractFaceFromFrame(pixelBuffer: pixelBuffer, face: winner) {
                DispatchQueue.main.async {
                    self.capturedWinnerImage = faceImage
                    print("âœ… Winner face captured successfully")
                }
            } else {
                print("âŒ Failed to extract winner face from frame")
            }
        }
    }
    
    private func extractFaceFromFrame(pixelBuffer: CVPixelBuffer, face: DetectedFace) -> UIImage? {
        // CVPixelBufferë¥¼ UIImageë¡œ ë³€í™˜
        let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
        let context = CIContext()
        
        // ì´ë¯¸ì§€ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
        let imageSize = ciImage.extent.size
        
        // ì–¼êµ´ ì˜ì—­ì„ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜
        var faceRect = face.boundingBox
        
        // Vision ì¢Œí‘œë¥¼ ì´ë¯¸ì§€ ì¢Œí‘œë¡œ ë³€í™˜ (Yì¶• ë°˜ì „)
        faceRect.origin.y = 1.0 - faceRect.origin.y - faceRect.size.height
        
        // ì „ë©´ ì¹´ë©”ë¼ì¸ ê²½ìš° Xì¶• ë¯¸ëŸ¬ë§
        if cameraPosition == .front {
            faceRect.origin.x = 1.0 - faceRect.origin.x - faceRect.size.width
        }
        
        // ì´ë¯¸ì§€ í¬ê¸°ì— ë§ê²Œ ìŠ¤ì¼€ì¼ë§
        let scaledRect = CGRect(
            x: faceRect.origin.x * imageSize.width,
            y: faceRect.origin.y * imageSize.height,
            width: faceRect.size.width * imageSize.width,
            height: faceRect.size.height * imageSize.height
        )
        
        // ì–¼êµ´ ì˜ì—­ì„ ì•½ê°„ í™•ëŒ€ (ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
        let padding = min(scaledRect.width, scaledRect.height) * 0.2
        let expandedRect = CGRect(
            x: max(0, scaledRect.origin.x - padding),
            y: max(0, scaledRect.origin.y - padding),
            width: min(imageSize.width - max(0, scaledRect.origin.x - padding), scaledRect.width + padding * 2),
            height: min(imageSize.height - max(0, scaledRect.origin.y - padding), scaledRect.height + padding * 2)
        )
        
        // ì–¼êµ´ ì˜ì—­ ìë¥´ê¸°
        let croppedCIImage = ciImage.cropped(to: expandedRect)
        
        // CGImageë¡œ ë³€í™˜
        guard let cgImage = context.createCGImage(croppedCIImage, from: croppedCIImage.extent) else {
            return nil
        }
        
        // UIImageë¡œ ë³€í™˜ (ë°©í–¥ ì˜¬ë°”ë¡œ ì„¤ì •)
        let uiImage = UIImage(cgImage: cgImage, scale: 1.0, orientation: getUIImageOrientation())
        
        return uiImage
    }
    
    private func getUIImageOrientation() -> UIImage.Orientation {
        switch cameraPosition {
        case .front:
            return .leftMirrored
        case .back:
            return .right
        default:
            return .right
        }
    }
    
    deinit {
        stopCamera()
    }
}

// MARK: - AVCaptureVideoDataOutputSampleBufferDelegate
extension LiveCameraRouletteController: AVCaptureVideoDataOutputSampleBufferDelegate {
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        
        // ì„±ëŠ¥ ìµœì í™”: ì¼ì • ê°„ê²©ìœ¼ë¡œë§Œ ì²˜ë¦¬
        let now = Date()
        guard now.timeIntervalSince(lastProcessTime) >= processingInterval else { return }
        lastProcessTime = now
        
        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer),
              let request = faceDetectionRequest else { return }
        
        // ìµœì‹  í”„ë ˆì„ ì €ì¥ (ë‹¹ì²¨ì ì–¼êµ´ ìº¡ì²˜ìš©)
        latestCameraFrame = pixelBuffer
        
        // ì¹´ë©”ë¼ ë°©í–¥ì— ë”°ë¥¸ ì´ë¯¸ì§€ ë°©í–¥ ì„¤ì •
        let imageOrientation: CGImagePropertyOrientation
        switch cameraPosition {
        case .front:
            // ì „ë©´ ì¹´ë©”ë¼: ë¯¸ëŸ¬ë§ ì²˜ë¦¬
            imageOrientation = .leftMirrored
        case .back:
            // í›„ë©´ ì¹´ë©”ë¼: ì¼ë°˜ ë°©í–¥
            imageOrientation = .right
        default:
            imageOrientation = .right
        }
        
        // ë¹„ë””ì˜¤ ì—°ê²° ë°©í–¥ ì„¤ì •
        if connection.isVideoOrientationSupported {
            connection.videoOrientation = .portrait
        }
        
        // ì „ë©´ ì¹´ë©”ë¼ì¼ ë•Œ ë¯¸ëŸ¬ë§ ì„¤ì •
        if cameraPosition == .front && connection.isVideoMirroringSupported {
            connection.isVideoMirrored = true
        }
        
        let imageRequestHandler = VNImageRequestHandler(
            cvPixelBuffer: pixelBuffer,
            orientation: imageOrientation,
            options: [:]
        )
        
        do {
            try imageRequestHandler.perform([request])
        } catch {
            print("âŒ Face detection failed: \(error)")
        }
    }
}
