//
//  FaceDetectionController.swift
//  UnluckyDraw
//
//  Created on 2025-06-16
//

import Foundation
import Vision
import UIKit
import CoreImage
import AVFoundation
import AudioToolbox

class FaceDetectionController: ObservableObject {
    @Published var detectedFaces: [DetectedFace] = []
    @Published var editableFaces: [EditableFace] = []  // üÜï Editable face list
    @Published var isProcessing = false
    @Published var error: FaceDetectionError?
    @Published var currentImageSize: CGSize = .zero    // üÜï Current image size
    
    private var faceDetectionRequest: VNDetectFaceRectanglesRequest?
    private var originalImage: UIImage?  // üÜï Store original image for manual box cropping
    
    enum FaceDetectionError: LocalizedError {
        case noFacesDetected
        case processingFailed
        case invalidImage
        
        var errorDescription: String? {
            switch self {
            case .noFacesDetected:
                return "No faces detected in the image"
            case .processingFailed:
                return "Failed to process the image"
            case .invalidImage:
                return "Invalid image provided"
            }
        }
    }
    
    init() {
        setupFaceDetection()
    }
    
    private func setupFaceDetection() {
        faceDetectionRequest = VNDetectFaceRectanglesRequest { [weak self] request, error in
            DispatchQueue.main.async {
                self?.processFaceDetectionResults(request: request, error: error)
            }
        }
        
        // Set face detection to maximum performance
        faceDetectionRequest?.revision = VNDetectFaceRectanglesRequestRevision3
        
        // Use GPU acceleration and performance optimization
        if #available(iOS 14.0, *) {
            faceDetectionRequest?.usesCPUOnly = false // Utilize GPU acceleration
        }
        

    }
    
    func detectFaces(in image: UIImage) {
        guard let cgImage = image.cgImage else {
            self.error = .invalidImage
            return
        }
        
        isProcessing = true
        error = nil
        detectedFaces.removeAll()
        

        
        // Keep image preprocessing as is, but preserve orientation information
        let processedImage = preprocessImageForDetection(cgImage)
        
        // Set Vision to automatically handle image orientation
        let imageOrientation = cgImageOrientationFromUIImage(image.imageOrientation)
        
        let imageRequestHandler = VNImageRequestHandler(
            cgImage: processedImage,
            orientation: imageOrientation, // Important: pass original orientation information
            options: [:]
        )
        

        
        // ‚≠êÔ∏è ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÎ•º Ï†ÄÏû• (ÎÇòÏ§ëÏóê ÏñºÍµ¥ ÌÅ¨Î°≠Ïö©)
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            guard let request = self?.faceDetectionRequest else { return }
            
            do {
                try imageRequestHandler.perform([request])
                
                // ‚≠êÔ∏è Crop all faces after Vision processing completes
                DispatchQueue.main.async {
                    self?.originalImage = image  // üÜï Store original image
                    self?.cropAllDetectedFaces(from: image)
                    
                    // üÜï If image size is set, immediately convert to editableFaces
                    if self?.currentImageSize != .zero {
                        self?.convertToEditableFaces(imageSize: self?.currentImageSize ?? .zero)
                    }
                }
            } catch {
                DispatchQueue.main.async {
                    self?.error = .processingFailed
                    self?.isProcessing = false
                }
            }
        }
    }
    
    // UIImage.OrientationÏùÑ CGImagePropertyOrientationÏúºÎ°ú Î≥ÄÌôò
    private func cgImageOrientationFromUIImage(_ uiOrientation: UIImage.Orientation) -> CGImagePropertyOrientation {
        switch uiOrientation {
        case .up: return .up
        case .down: return .down
        case .left: return .left
        case .right: return .right
        case .upMirrored: return .upMirrored
        case .downMirrored: return .downMirrored
        case .leftMirrored: return .leftMirrored
        case .rightMirrored: return .rightMirrored
        @unknown default: return .up
        }
    }
    
    private func preprocessImageForDetection(_ cgImage: CGImage) -> CGImage {
        let context = CIContext(options: [.useSoftwareRenderer: false]) // GPU ÏÇ¨Ïö©
        let ciImage = CIImage(cgImage: cgImage)
        
        // Multi-stage image enhancement pipeline
        
        // Stage 1: Basic color correction
        let colorCorrected = ciImage
            .applyingFilter("CIColorControls", parameters: [
                "inputContrast": 1.3,      // Increase contrast
                "inputBrightness": 0.15,   // Slightly increase brightness
                "inputSaturation": 0.8     // Slightly decrease saturation
            ])
        
        // Stage 2: Sharpening (sharpen face contours)
        let sharpened = colorCorrected
            .applyingFilter("CISharpenLuminance", parameters: [
                "inputSharpness": 0.7
            ])
        
        // Stage 3: Noise reduction
        let denoised = sharpened
            .applyingFilter("CINoiseReduction", parameters: [
                "inputNoiseLevel": 0.02,
                "inputSharpness": 0.9
            ])
        
        // Stage 4: Gamma correction (clarify face areas)
        let gammaAdjusted = denoised
            .applyingFilter("CIGammaAdjust", parameters: [
                "inputPower": 0.85
            ])
        
        // Stage 5: Color temperature normalization (natural skin tone)
        let temperatureAdjusted = gammaAdjusted
            .applyingFilter("CITemperatureAndTint", parameters: [
                "inputNeutral": CIVector(x: 6500, y: 0),
                "inputTargetNeutral": CIVector(x: 6500, y: 0)
            ])
        
        // Generate final image
        if let outputCGImage = context.createCGImage(temperatureAdjusted, from: temperatureAdjusted.extent) {
            print("‚ú® Image preprocessing completed with \(outputCGImage.width)x\(outputCGImage.height) resolution")
            return outputCGImage
        }
        
        print("‚ö†Ô∏è Image preprocessing failed, using original")
        return cgImage
    }
    
    private func processFaceDetectionResults(request: VNRequest, error: Error?) {
        defer { isProcessing = false }
        
        if let error = error {
            self.error = .processingFailed
            return
        }
        
        guard let results = request.results as? [VNFaceObservation] else {
            self.error = .processingFailed
            return
        }
        
        if results.isEmpty {
            self.error = .noFacesDetected
            return
        }
        
        // Ïã†Î¢∞ÎèÑ Î∞è ÌÅ¨Í∏∞ Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ (Îçî ÏóÑÍ≤©Ìïú Í∏∞Ï§Ä)
        let faces = results.compactMap { observation -> DetectedFace? in
            let boundingBox = observation.boundingBox
            let confidence = observation.confidence
            
            // 1. Ïã†Î¢∞ÎèÑ Í≤ÄÏÇ¨ (Îçî ÏóÑÍ≤©ÌïòÍ≤å)
            guard confidence > 0.4 else {
                return nil
            }
            
            // 2. ÏñºÍµ¥ ÌÅ¨Í∏∞ Í≤ÄÏÇ¨ (ÎÑàÎ¨¥ ÏûëÏùÄ ÏñºÍµ¥ Ï†úÏô∏)
            let faceArea = boundingBox.width * boundingBox.height
            guard faceArea > 0.01 else { // Ï†ÑÏ≤¥ Ïù¥ÎØ∏ÏßÄÏùò 1% Ïù¥ÏÉÅ
                return nil
            }
            
            // 3. ÏñºÍµ¥ ÎπÑÏú® Í≤ÄÏÇ¨ (ÎÑàÎ¨¥ Í∏∏Í±∞ÎÇò ÎÑ©ÏùÄ ÏñºÍµ¥ Ï†úÏô∏)
            let aspectRatio = boundingBox.width / boundingBox.height
            guard aspectRatio > 0.5 && aspectRatio < 2.0 else {
                return nil
            }
            
            // 4. ÏñºÍµ¥ ÏúÑÏπò Í≤ÄÏÇ¨ (Ïù¥ÎØ∏ÏßÄ Î∞ñÏúºÎ°ú ÎÑàÎ¨¥ ÎßéÏù¥ ÎÇòÍ∞Ñ ÏñºÍµ¥ Ï†úÏô∏)
            guard boundingBox.minX >= -0.1 && boundingBox.maxX <= 1.1 &&
                  boundingBox.minY >= -0.1 && boundingBox.maxY <= 1.1 else {
                return nil
            }
            
            // ‚≠êÔ∏è Vision Ï¢åÌëúÍ≥ÑÎ•º Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ (YÏ∂ï Î≥ÄÌôòÌïòÏßÄ ÏïäÏùå)
            let face = DetectedFace(
                boundingBox: boundingBox, // Vision ÏõêÎ≥∏ Ï¢åÌëú Í∑∏ÎåÄÎ°ú Ï†ÄÏû•
                confidence: confidence
            )
            

            
            return face
        }
        
        // ÌïÑÌÑ∞ÎßÅ ÌõÑÏóêÎèÑ ÏñºÍµ¥Ïù¥ ÏóÜÏúºÎ©¥ ÏóêÎü¨
        if faces.isEmpty {
            self.error = .noFacesDetected
            return
        }
        
        // üé∞ ÏõêÎûò Î°úÏßÅ Ïú†ÏßÄÌïòÎêò, UIÎßå Ï†êÏßÑÏ†ÅÏúºÎ°ú ÏóÖÎç∞Ïù¥Ìä∏
        self.detectedFaces = faces
        
        // UI Ïï†ÎãàÎ©îÏù¥ÏÖòÏùÑ ÏúÑÌïú Î≥ÑÎèÑ Ï≤òÎ¶¨ (Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞Îäî Í∑∏ÎåÄÎ°ú)
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
            // ÎßàÏßÄÎßâÏóê ÏôÑÎ£å ÏÇ¨Ïö¥ÎìúÎßå
            SoundManager.shared.playCompleteSound()
        }
        

    }
    
    // ‚≠êÔ∏è ÏôÑÏ†ÑÌûà ÏÉàÎ°úÏö¥ ÏñºÍµ¥ ÌÅ¨Î°≠ ÏãúÏä§ÌÖú
    private func cropAllDetectedFaces(from originalImage: UIImage) {
        
        // Ïù¥ÎØ∏ÏßÄÎ•º Ï†ïÍ∑úÌôîÎêú Î∞©Ìñ•ÏúºÎ°ú Î≥ÄÌôò
        let normalizedImage = normalizeImageOrientation(originalImage)
        
        // Î™®Îì† ÏñºÍµ¥ÏùÑ ÌÅ¨Î°≠ÌïòÏó¨ Ï†ÄÏû•
        for (index, face) in detectedFaces.enumerated() {
            if let croppedImage = advancedFaceCrop(from: normalizedImage, face: face) {
                detectedFaces[index].croppedImage = croppedImage
            }
        }
    }
    
    // Ïù¥ÎØ∏ÏßÄ Î∞©Ìñ•ÏùÑ Ï†ïÍ∑úÌôî (Ìï≠ÏÉÅ .up ÏÉÅÌÉúÎ°ú ÎßåÎì§Í∏∞)
    private func normalizeImageOrientation(_ image: UIImage) -> UIImage {
        // Ïù¥ÎØ∏ÏßÄÍ∞Ä Ïù¥ÎØ∏ Ï†ïÏÉÅ Î∞©Ìñ•Ïù¥Î©¥ Í∑∏ÎåÄÎ°ú Î∞òÌôò
        if image.imageOrientation == .up {
            return image
        }
        
        // Ï†ïÍ∑úÌôîÎêú ÌÅ¨Í∏∞ Í≥ÑÏÇ∞
        let normalizedSize = CGSize(
            width: image.size.width,
            height: image.size.height
        )
        
        // Ï†ïÍ∑úÌôîÎêú Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ±
        UIGraphicsBeginImageContextWithOptions(normalizedSize, false, image.scale)
        defer { UIGraphicsEndImageContext() }
        
        image.draw(in: CGRect(origin: .zero, size: normalizedSize))
        
        if let normalizedImage = UIGraphicsGetImageFromCurrentImageContext() {
            return normalizedImage
        }
        return image
    }
    
    // Í≥†Í∏â ÏñºÍµ¥ ÌÅ¨Î°≠ Ìï®Ïàò
    private func advancedFaceCrop(from image: UIImage, face: DetectedFace) -> UIImage? {
        guard let cgImage = image.cgImage else {
            return nil
        }
        
        let imageWidth = CGFloat(cgImage.width)
        let imageHeight = CGFloat(cgImage.height)
        
        // Vision Ï¢åÌëúÍ≥ÑÎ•º CGImage Ï¢åÌëúÍ≥ÑÎ°ú Î≥ÄÌôò
        // Vision: Ï¢åÌïòÎã® ÏõêÏ†ê (0,0), YÏ∂ï ÏúÑÏ™ΩÏù¥ +
        // CGImage: Ï¢åÏÉÅÎã® ÏõêÏ†ê (0,0), YÏ∂ï ÏïÑÎûòÏ™ΩÏù¥ +
        let visionBox = face.boundingBox
        
        let cgBox = CGRect(
            x: visionBox.minX * imageWidth,
            y: (1.0 - visionBox.maxY) * imageHeight, // YÏ∂ï Îí§ÏßëÍ∏∞
            width: visionBox.width * imageWidth,
            height: visionBox.height * imageHeight
        )
        

        
        // ÏñºÍµ¥ ÏòÅÏó≠ÏùÑ 20% ÌôïÏû• (ÏïàÏ†ÑÌïòÍ≥† ÏûêÏó∞Ïä§Îü¨Ïö¥ ÌÅ¨Î°≠)
        let expandRatio: CGFloat = 0.2
        let expandX = cgBox.width * expandRatio
        let expandY = cgBox.height * expandRatio
        
        let expandedBox = CGRect(
            x: max(0, cgBox.minX - expandX),
            y: max(0, cgBox.minY - expandY),
            width: min(imageWidth - max(0, cgBox.minX - expandX), cgBox.width + expandX * 2),
            height: min(imageHeight - max(0, cgBox.minY - expandY), cgBox.height + expandY * 2)
        )
        

        
        // Í≤ΩÍ≥Ñ Í≤ÄÏÇ¨
        guard expandedBox.width > 0 && expandedBox.height > 0 &&
              expandedBox.minX >= 0 && expandedBox.minY >= 0 &&
              expandedBox.maxX <= imageWidth && expandedBox.maxY <= imageHeight else {
            return nil
        }
        
        // Ïù¥ÎØ∏ÏßÄ ÌÅ¨Î°≠
        guard let croppedCGImage = cgImage.cropping(to: expandedBox) else {
            return nil
        }
        
        // ÌÅ¨Î°≠Îêú Ïù¥ÎØ∏ÏßÄÍ∞Ä ÎÑàÎ¨¥ ÏûëÏúºÎ©¥ ÌôïÎåÄ
        let minSize: CGFloat = 200
        let croppedImage = UIImage(cgImage: croppedCGImage, scale: 1.0, orientation: .up)
        
        if max(croppedImage.size.width, croppedImage.size.height) < minSize {
            let scale = minSize / max(croppedImage.size.width, croppedImage.size.height)
            let newSize = CGSize(
                width: croppedImage.size.width * scale,
                height: croppedImage.size.height * scale
            )
            
            UIGraphicsBeginImageContextWithOptions(newSize, false, 1.0)
            defer { UIGraphicsEndImageContext() }
            
            croppedImage.draw(in: CGRect(origin: .zero, size: newSize))
            
            if let scaledImage = UIGraphicsGetImageFromCurrentImageContext() {
                return scaledImage
            }
        }
        

        return croppedImage
    }
    
    func clearResults() {
        detectedFaces.removeAll()
        editableFaces.removeAll()
        error = nil
        isProcessing = false
        currentImageSize = .zero
    }
    
    // MARK: - üÜï ÏñºÍµ¥ Ìé∏Ïßë Í∏∞Îä•
    
    /// ÏñºÍµ¥ Ïù∏Ïãù Í≤∞Í≥ºÎ•º Ìé∏Ïßë Í∞ÄÎä•Ìïú ÏñºÍµ¥Î°ú Î≥ÄÌôò
    func convertToEditableFaces(imageSize: CGSize) {
        currentImageSize = imageSize
        editableFaces = detectedFaces.map { face in
            EditableFace(from: face, imageSize: imageSize)
        }
    }
    
    /// ÏÉàÎ°úÏö¥ ÏñºÍµ¥ Î∞ïÏä§ Ï∂îÍ∞Ä (Ìñ•ÏÉÅÎêú Î≤ÑÏ†Ñ)
    func addNewFace() {
        guard currentImageSize != .zero else {
            return
        }
        
        // Îçî ÎòëÎòëÌïú ÌÅ¨Í∏∞ Í≥ÑÏÇ∞
        let smartSize = calculateSmartBoxSize()
        let suggestedPosition = EditableFace.suggestPosition(
            for: smartSize,
            in: currentImageSize,
            avoiding: editableFaces
        )
        
        var newFace = EditableFace(
            boundingBox: CGRect(
                origin: suggestedPosition,
                size: smartSize
            ),
            confidence: 1.0,
            isUserAdded: true
        )
        
        // üÜï ÏàòÎèô Î∞ïÏä§ Ï∂îÍ∞Ä Ïãú Ï¶âÏãú ÌÅ¨Î°≠ Ïã§Ìñâ
        newFace.croppedImage = cropFaceFromEditableBox(newFace)
        
        editableFaces.append(newFace)
        
        // ÏãúÍ∞ÅÏ†Å ÌîºÎìúÎ∞±ÏùÑ ÏúÑÌï¥ Ïû†Ïãú ÌïòÏù¥ÎùºÏù¥Ìä∏
        if let newIndex = editableFaces.firstIndex(where: { $0.id == newFace.id }) {
            editableFaces[newIndex].isHighlighted = true
            
            // 2Ï¥à ÌõÑ ÌïòÏù¥ÎùºÏù¥Ìä∏ Ìï¥Ï†ú
            DispatchQueue.main.asyncAfter(deadline: .now() + 2.0) {
                if newIndex < self.editableFaces.count {
                    self.editableFaces[newIndex].isHighlighted = false
                }
            }
        }
    }
    
    /// ÎòëÎòëÌïú Î∞ïÏä§ ÌÅ¨Í∏∞ Í≥ÑÏÇ∞
    private func calculateSmartBoxSize() -> CGSize {
        if editableFaces.isEmpty {
            // Ï≤´ Î≤àÏß∏ Î∞ïÏä§Ïù∏ Í≤ΩÏö∞ Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞Ïóê ÎπÑÎ°ÄÌïú Í∏∞Î≥∏ ÌÅ¨Í∏∞
            let defaultRatio: CGFloat = 0.15 // Ïù¥ÎØ∏ÏßÄÏùò 15%
            let size = min(currentImageSize.width, currentImageSize.height) * defaultRatio
            return CGSize(width: size, height: size * 1.2) // ÏïΩÍ∞Ñ ÏÑ∏Î°úÎ°ú Í∏¥ ÌòïÌÉú
        }
        
        // Í∏∞Ï°¥ Î∞ïÏä§Îì§Ïùò ÌèâÍ∑† ÌÅ¨Í∏∞ Í≥ÑÏÇ∞
        let averageSize = EditableFace.averageSize(from: editableFaces)
        
        // ÌÅ¨Í∏∞ Î≤îÏúÑ Ï†úÌïú (ÎÑàÎ¨¥ ÏûëÍ±∞ÎÇò ÌÅ¨ÏßÄ ÏïäÎèÑÎ°ù)
        let minSize: CGFloat = 60
        let maxSize = min(currentImageSize.width, currentImageSize.height) * 0.3
        
        let clampedWidth = max(minSize, min(maxSize, averageSize.width))
        let clampedHeight = max(minSize, min(maxSize, averageSize.height))
        
        return CGSize(width: clampedWidth, height: clampedHeight)
    }
    
    /// ÏñºÍµ¥ Î∞ïÏä§ ÏÇ≠Ï†ú
    func removeFace(withId id: UUID) {
        guard editableFaces.count > 1 else {
            return
        }
        
        if let index = editableFaces.firstIndex(where: { $0.id == id }) {
            editableFaces.remove(at: index)
        }
    }
    
    /// ÏñºÍµ¥ Î∞ïÏä§ ÏúÑÏπò ÏóÖÎç∞Ïù¥Ìä∏ (Ìñ•ÏÉÅÎêú Î≤ÑÏ†Ñ)
    func updateFacePosition(id: UUID, dragOffset: CGSize) {
        if let index = editableFaces.firstIndex(where: { $0.id == id }) {
            editableFaces[index].dragOffset = dragOffset
            editableFaces[index].isDragging = true
        }
    }
    
    /// ÎìúÎûòÍ∑∏ ÏôÑÎ£å Ïãú ÏúÑÏπò Ï†ÅÏö© (Ìñ•ÏÉÅÎêú Î≤ÑÏ†Ñ)
    func finalizeFacePosition(id: UUID) {
        if let index = editableFaces.firstIndex(where: { $0.id == id }) {
            // Í∏∞Ï°¥ Î°úÏßÅ Ïú†ÏßÄÌïòÎ©¥ÏÑú Ï∂îÍ∞Ä Í≤ÄÏ¶ù
            editableFaces[index].applyDragOffset()
            editableFaces[index].constrainToImage(size: currentImageSize)
            
            // üÜï ÏÇ¨Ïö©Ïûê Ï∂îÍ∞Ä Î∞ïÏä§Í∞Ä Ïù¥ÎèôÌñàÏúºÎ©¥ Ïû¨ÌÅ¨Î°≠
            if editableFaces[index].isUserAdded {
                editableFaces[index].croppedImage = cropFaceFromEditableBox(editableFaces[index])
            }
        }
    }
    
    /// Î∞ïÏä§Í∞Ä Ïù¥ÎØ∏ÏßÄ Í≤ΩÍ≥Ñ ÎÇ¥Ïóê ÏûàÎäîÏßÄ ÌôïÏù∏
    private func isWithinBounds(_ box: CGRect) -> Bool {
        let imageBounds = CGRect(origin: .zero, size: currentImageSize)
        return imageBounds.contains(box)
    }
    
    /// Ìé∏ÏßëÎêú ÏñºÍµ¥Îì§ÏùÑ DetectedFace ÌòïÌÉúÎ°ú Î≥ÄÌôò (Î£∞Î†õÏö©)
    func getEditedFacesAsDetected() -> [DetectedFace] {
        return editableFaces.map { editableFace in
            // ÌîΩÏÖÄ Ï¢åÌëúÎ•º Vision Ï¢åÌëúÎ°ú Ïó≠Î≥ÄÌôò
            let visionBox = CGRect(
                x: editableFace.boundingBox.minX / currentImageSize.width,
                y: 1.0 - (editableFace.boundingBox.maxY / currentImageSize.height),
                width: editableFace.boundingBox.width / currentImageSize.width,
                height: editableFace.boundingBox.height / currentImageSize.height
            )
            
            var detectedFace = DetectedFace(
                boundingBox: visionBox,
                confidence: editableFace.confidence
            )
            
            // üÜï ÌÅ¨Î°≠ Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÏúºÎ©¥ Ï¶âÏÑùÏóêÏÑú ÏÉùÏÑ± (ÏïàÏ†ÑÏû•Ïπò)
            if let croppedImage = editableFace.croppedImage {
                detectedFace.croppedImage = croppedImage
            } else if editableFace.isUserAdded {
                detectedFace.croppedImage = cropFaceFromEditableBox(editableFace)
            }
            
            return detectedFace
        }
    }
    
    // MARK: - üÜï ÏàòÎèô Î∞ïÏä§ ÌÅ¨Î°≠ ÏãúÏä§ÌÖú
    
    /// EditableFace Î∞ïÏä§ ÏòÅÏó≠ÏùÑ ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄÏóêÏÑú ÌÅ¨Î°≠
    private func cropFaceFromEditableBox(_ editableFace: EditableFace) -> UIImage? {
        guard let originalImage = originalImage else {
            return nil
        }
        
        guard let cgImage = originalImage.cgImage else {
            return nil
        }
        
        let imageWidth = CGFloat(cgImage.width)
        let imageHeight = CGFloat(cgImage.height)
        let boxInPixels = editableFace.boundingBox
        
        // ÎîîÏä§ÌîåÎ†àÏù¥ Ï¢åÌëúÎ•º Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄ Ï¢åÌëúÎ°ú Î≥ÄÌôò
        let scaleX = imageWidth / currentImageSize.width
        let scaleY = imageHeight / currentImageSize.height
        
        let cropBox = CGRect(
            x: boxInPixels.minX * scaleX,
            y: boxInPixels.minY * scaleY,
            width: boxInPixels.width * scaleX,
            height: boxInPixels.height * scaleY
        )
        

        
        // Í≤ΩÍ≥Ñ Í≤ÄÏÇ¨
        let safeCropBox = CGRect(
            x: max(0, cropBox.minX),
            y: max(0, cropBox.minY),
            width: min(imageWidth - max(0, cropBox.minX), cropBox.width),
            height: min(imageHeight - max(0, cropBox.minY), cropBox.height)
        )
        
        guard safeCropBox.width > 0 && safeCropBox.height > 0 else {
            return nil
        }
        
        // Ïù¥ÎØ∏ÏßÄ ÌÅ¨Î°≠
        guard let croppedCGImage = cgImage.cropping(to: safeCropBox) else {
            return nil
        }
        
        let croppedImage = UIImage(cgImage: croppedCGImage, scale: 1.0, orientation: .up)
        
        return croppedImage
    }
}
