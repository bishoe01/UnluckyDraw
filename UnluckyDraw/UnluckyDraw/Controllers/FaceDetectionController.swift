//
//  FaceDetectionController.swift
//  UnluckyDraw
//
//  Created on 2025-06-16
//

import Foundation
import Vision
import UIKit
import CoreImage
import AVFoundation
import AudioToolbox

class FaceDetectionController: ObservableObject {
    @Published var detectedFaces: [DetectedFace] = []
    @Published var editableFaces: [EditableFace] = []  // ğŸ†• í¸ì§‘ ê°€ëŠ¥í•œ ì–¼êµ´ ëª©ë¡
    @Published var isProcessing = false
    @Published var error: FaceDetectionError?
    @Published var currentImageSize: CGSize = .zero    // ğŸ†• í˜„ì¬ ì´ë¯¸ì§€ í¬ê¸°
    
    private var faceDetectionRequest: VNDetectFaceRectanglesRequest?
    
    enum FaceDetectionError: LocalizedError {
        case noFacesDetected
        case processingFailed
        case invalidImage
        
        var errorDescription: String? {
            switch self {
            case .noFacesDetected:
                return "No faces detected in the image"
            case .processingFailed:
                return "Failed to process the image"
            case .invalidImage:
                return "Invalid image provided"
            }
        }
    }
    
    init() {
        setupFaceDetection()
    }
    
    private func setupFaceDetection() {
        faceDetectionRequest = VNDetectFaceRectanglesRequest { [weak self] request, error in
            DispatchQueue.main.async {
                self?.processFaceDetectionResults(request: request, error: error)
            }
        }
        
        // ìµœëŒ€ ì„±ëŠ¥ìœ¼ë¡œ ì–¼êµ´ ì¸ì‹ ì„¤ì •
        faceDetectionRequest?.revision = VNDetectFaceRectanglesRequestRevision3
        
        // GPU ê°€ì† ì‚¬ìš© ë° ì„±ëŠ¥ ìµœì í™”
        if #available(iOS 14.0, *) {
            faceDetectionRequest?.usesCPUOnly = false // GPU ê°€ì† í™œìš©
        }
        
        print("ğŸ¤– Face Detection initialized with max performance settings")
    }
    
    func detectFaces(in image: UIImage) {
        guard let cgImage = image.cgImage else {
            self.error = .invalidImage
            return
        }
        
        isProcessing = true
        error = nil
        detectedFaces.removeAll()
        
        print("ğŸ” Processing image for face detection:")
        print("  Original size: \(image.size)")
        print("  Original orientation: \(image.imageOrientation.rawValue)")
        
        // ì´ë¯¸ì§€ ì „ì²˜ë¦¬ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì§€ë§Œ, ë°©í–¥ ì •ë³´ë¥¼ ë³´ì¡´
        let processedImage = preprocessImageForDetection(cgImage)
        
        // Visionì´ ì´ë¯¸ì§€ ë°©í–¥ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ë„ë¡ ì„¤ì •
        let imageOrientation = cgImageOrientationFromUIImage(image.imageOrientation)
        
        let imageRequestHandler = VNImageRequestHandler(
            cgImage: processedImage,
            orientation: imageOrientation, // ì¤‘ìš”: ì›ë³¸ ë°©í–¥ ì •ë³´ ì „ë‹¬
            options: [:]
        )
        
        print("ğŸ” Vision processing with orientation: \(imageOrientation.rawValue)")
        
        // â­ï¸ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì €ì¥ (ë‚˜ì¤‘ì— ì–¼êµ´ í¬ë¡­ìš©)
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            guard let request = self?.faceDetectionRequest else { return }
            
            do {
                try imageRequestHandler.perform([request])
                
                // â­ï¸ Vision ì²˜ë¦¬ ì™„ë£Œ í›„ ëª¨ë“  ì–¼êµ´ í¬ë¡­
                DispatchQueue.main.async {
                    self?.cropAllDetectedFaces(from: image)
                    
                    // ğŸ†• ì´ë¯¸ì§€ í¬ê¸°ê°€ ì„¤ì •ë˜ì–´ ìˆë‹¤ë©´ ì¦‰ì‹œ editableFacesë¡œ ë³€í™˜
                    if self?.currentImageSize != .zero {
                        self?.convertToEditableFaces(imageSize: self?.currentImageSize ?? .zero)
                    }
                }
            } catch {
                DispatchQueue.main.async {
                    print("âŒ Face detection failed: \(error)")
                    self?.error = .processingFailed
                    self?.isProcessing = false
                }
            }
        }
    }
    
    // UIImage.Orientationì„ CGImagePropertyOrientationìœ¼ë¡œ ë³€í™˜
    private func cgImageOrientationFromUIImage(_ uiOrientation: UIImage.Orientation) -> CGImagePropertyOrientation {
        switch uiOrientation {
        case .up: return .up
        case .down: return .down
        case .left: return .left
        case .right: return .right
        case .upMirrored: return .upMirrored
        case .downMirrored: return .downMirrored
        case .leftMirrored: return .leftMirrored
        case .rightMirrored: return .rightMirrored
        @unknown default: return .up
        }
    }
    
    private func preprocessImageForDetection(_ cgImage: CGImage) -> CGImage {
        let context = CIContext(options: [.useSoftwareRenderer: false]) // GPU ì‚¬ìš©
        let ciImage = CIImage(cgImage: cgImage)
        
        // ë‹¤ë‹¨ê³„ ì´ë¯¸ì§€ í–¥ìƒ íŒŒì´í”„ë¼ì¸
        
        // 1ë‹¨ê³„: ê¸°ë³¸ ìƒ‰ìƒ ë³´ì •
        let colorCorrected = ciImage
            .applyingFilter("CIColorControls", parameters: [
                "inputContrast": 1.3,      // ëŒ€ë¹„ ì¦ê°€
                "inputBrightness": 0.15,   // ë°ê¸° ì•½ê°„ ì¦ê°€
                "inputSaturation": 0.8     // ì±„ë„ ì•½ê°„ ê°ì†Œ
            ])
        
        // 2ë‹¨ê³„: ìƒ¤í”„ë‹ (ì–¼êµ´ ìœ¤ê³½ ì„ ëª…í•˜ê²Œ)
        let sharpened = colorCorrected
            .applyingFilter("CISharpenLuminance", parameters: [
                "inputSharpness": 0.7
            ])
        
        // 3ë‹¨ê³„: ë…¸ì´ì¦ˆ ì œê±°
        let denoised = sharpened
            .applyingFilter("CINoiseReduction", parameters: [
                "inputNoiseLevel": 0.02,
                "inputSharpness": 0.9
            ])
        
        // 4ë‹¨ê³„: ê°ë§ˆ ë³´ì • (ì–¼êµ´ ì˜ì—­ ëª…í™•í•˜ê²Œ)
        let gammaAdjusted = denoised
            .applyingFilter("CIGammaAdjust", parameters: [
                "inputPower": 0.85
            ])
        
        // 5ë‹¨ê³„: ìƒ‰ì˜¨ ì •ê·œí™” (ìì—°ìŠ¤ëŸ¬ìš´ í”¼ë¶€í†¤ ì—°ì¶œ)
        let temperatureAdjusted = gammaAdjusted
            .applyingFilter("CITemperatureAndTint", parameters: [
                "inputNeutral": CIVector(x: 6500, y: 0),
                "inputTargetNeutral": CIVector(x: 6500, y: 0)
            ])
        
        // ìµœì¢… ì´ë¯¸ì§€ ìƒì„±
        if let outputCGImage = context.createCGImage(temperatureAdjusted, from: temperatureAdjusted.extent) {
            print("âœ¨ Image preprocessing completed with \(outputCGImage.width)x\(outputCGImage.height) resolution")
            return outputCGImage
        }
        
        print("âš ï¸ Image preprocessing failed, using original")
        return cgImage
    }
    
    private func processFaceDetectionResults(request: VNRequest, error: Error?) {
        defer { isProcessing = false }
        
        if let error = error {
            self.error = .processingFailed
            print("Face detection error: \(error.localizedDescription)")
            return
        }
        
        guard let results = request.results as? [VNFaceObservation] else {
            self.error = .processingFailed
            return
        }
        
        if results.isEmpty {
            self.error = .noFacesDetected
            print("âš ï¸ No faces detected in image")
            return
        }
        
        // ì‹ ë¢°ë„ ë° í¬ê¸° ê¸°ë°˜ í•„í„°ë§ (ë” ì—„ê²©í•œ ê¸°ì¤€)
        let faces = results.compactMap { observation -> DetectedFace? in
            let boundingBox = observation.boundingBox
            let confidence = observation.confidence
            
            // 1. ì‹ ë¢°ë„ ê²€ì‚¬ (ë” ì—„ê²©í•˜ê²Œ)
            guard confidence > 0.4 else {
                print("âŒ Rejected face with low confidence: \(String(format: "%.2f", confidence))")
                return nil
            }
            
            // 2. ì–¼êµ´ í¬ê¸° ê²€ì‚¬ (ë„ˆë¬´ ì‘ì€ ì–¼êµ´ ì œì™¸)
            let faceArea = boundingBox.width * boundingBox.height
            guard faceArea > 0.01 else { // ì „ì²´ ì´ë¯¸ì§€ì˜ 1% ì´ìƒ
                print("âŒ Rejected face with small area: \(String(format: "%.4f", faceArea))")
                return nil
            }
            
            // 3. ì–¼êµ´ ë¹„ìœ¨ ê²€ì‚¬ (ë„ˆë¬´ ê¸¸ê±°ë‚˜ ë„©ì€ ì–¼êµ´ ì œì™¸)
            let aspectRatio = boundingBox.width / boundingBox.height
            guard aspectRatio > 0.5 && aspectRatio < 2.0 else {
                print("âŒ Rejected face with invalid aspect ratio: \(String(format: "%.2f", aspectRatio))")
                return nil
            }
            
            // 4. ì–¼êµ´ ìœ„ì¹˜ ê²€ì‚¬ (ì´ë¯¸ì§€ ë°–ìœ¼ë¡œ ë„ˆë¬´ ë§ì´ ë‚˜ê°„ ì–¼êµ´ ì œì™¸)
            guard boundingBox.minX >= -0.1 && boundingBox.maxX <= 1.1 &&
                  boundingBox.minY >= -0.1 && boundingBox.maxY <= 1.1 else {
                print("âŒ Rejected face outside image bounds")
                return nil
            }
            
            // â­ï¸ Vision ì¢Œí‘œê³„ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€ (Yì¶• ë³€í™˜í•˜ì§€ ì•ŠìŒ)
            let face = DetectedFace(
                boundingBox: boundingBox, // Vision ì›ë³¸ ì¢Œí‘œ ê·¸ëŒ€ë¡œ ì €ì¥
                confidence: confidence
            )
            
            print("âœ… Accepted face: confidence=\(String(format: "%.2f", confidence)), area=\(String(format: "%.4f", faceArea)), ratio=\(String(format: "%.2f", aspectRatio))")
            
            return face
        }
        
        // í•„í„°ë§ í›„ì—ë„ ì–¼êµ´ì´ ì—†ìœ¼ë©´ ì—ëŸ¬
        if faces.isEmpty {
            self.error = .noFacesDetected
            print("âš ï¸ No valid faces found after filtering")
            if !results.isEmpty {
                print("  Original detections were filtered out due to quality criteria")
            }
            return
        }
        
        // ğŸ° ì›ë˜ ë¡œì§ ìœ ì§€í•˜ë˜, UIë§Œ ì ì§„ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        self.detectedFaces = faces
        
        // UI ì• ë‹ˆë©”ì´ì…˜ì„ ìœ„í•œ ë³„ë„ ì²˜ë¦¬ (ì‹¤ì œ ë°ì´í„°ëŠ” ê·¸ëŒ€ë¡œ)
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
            // ë§ˆì§€ë§‰ì— ì™„ë£Œ ì‚¬ìš´ë“œë§Œ
            SoundManager.shared.playCompleteSound()
        }
        
        // ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        print("ğŸ¯ Face Detection Results:")
        print("  â€¢ Total detected: \(results.count)")
        print("  â€¢ Filtered faces: \(faces.count)")
        
        for (index, face) in faces.enumerated() {
            print("  Face \(index + 1): confidence=\(String(format: "%.2f", face.confidence)), area=\(String(format: "%.4f", face.boundingBox.width * face.boundingBox.height))")
        }
        
        print("âœ… Face detection completed successfully")
    }
    
    // â­ï¸ ì™„ì „íˆ ìƒˆë¡œìš´ ì–¼êµ´ í¬ë¡­ ì‹œìŠ¤í…œ
    private func cropAllDetectedFaces(from originalImage: UIImage) {
        print("âœ‚ï¸ Starting advanced face cropping for \(detectedFaces.count) faces")
        
        // ì´ë¯¸ì§€ë¥¼ ì •ê·œí™”ëœ ë°©í–¥ìœ¼ë¡œ ë³€í™˜
        let normalizedImage = normalizeImageOrientation(originalImage)
        
        // ëª¨ë“  ì–¼êµ´ì„ í¬ë¡­í•˜ì—¬ ì €ì¥
        for (index, face) in detectedFaces.enumerated() {
            if let croppedImage = advancedFaceCrop(from: normalizedImage, face: face) {
                detectedFaces[index].croppedImage = croppedImage
                print("âœ… Face \(index + 1) cropped successfully: \(croppedImage.size)")
            } else {
                print("âŒ Failed to crop face \(index + 1)")
            }
        }
        
        print("ğŸ‰ Advanced face cropping completed! Ready for roulette!")
    }
    
    // ì´ë¯¸ì§€ ë°©í–¥ì„ ì •ê·œí™” (í•­ìƒ .up ìƒíƒœë¡œ ë§Œë“¤ê¸°)
    private func normalizeImageOrientation(_ image: UIImage) -> UIImage {
        // ì´ë¯¸ì§€ê°€ ì´ë¯¸ ì •ìƒ ë°©í–¥ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if image.imageOrientation == .up {
            return image
        }
        
        // ì •ê·œí™”ëœ í¬ê¸° ê³„ì‚°
        let normalizedSize = CGSize(
            width: image.size.width,
            height: image.size.height
        )
        
        // ì •ê·œí™”ëœ ì´ë¯¸ì§€ ìƒì„±
        UIGraphicsBeginImageContextWithOptions(normalizedSize, false, image.scale)
        defer { UIGraphicsEndImageContext() }
        
        image.draw(in: CGRect(origin: .zero, size: normalizedSize))
        
        if let normalizedImage = UIGraphicsGetImageFromCurrentImageContext() {
            print("ğŸ“ Image orientation normalized: \(image.imageOrientation.rawValue) â†’ \(normalizedImage.imageOrientation.rawValue)")
            return normalizedImage
        }
        
        print("âš ï¸ Failed to normalize image orientation, using original")
        return image
    }
    
    // ê³ ê¸‰ ì–¼êµ´ í¬ë¡­ í•¨ìˆ˜
    private func advancedFaceCrop(from image: UIImage, face: DetectedFace) -> UIImage? {
        guard let cgImage = image.cgImage else {
            print("âŒ Cannot get CGImage from UIImage")
            return nil
        }
        
        let imageWidth = CGFloat(cgImage.width)
        let imageHeight = CGFloat(cgImage.height)
        
        print("ğŸ” Image dimensions: \(imageWidth) x \(imageHeight)")
        print("ğŸ” Vision bounding box: \(face.boundingBox)")
        
        // Vision ì¢Œí‘œê³„ë¥¼ CGImage ì¢Œí‘œê³„ë¡œ ë³€í™˜
        // Vision: ì¢Œí•˜ë‹¨ ì›ì  (0,0), Yì¶• ìœ„ìª½ì´ +
        // CGImage: ì¢Œìƒë‹¨ ì›ì  (0,0), Yì¶• ì•„ë˜ìª½ì´ +
        let visionBox = face.boundingBox
        
        let cgBox = CGRect(
            x: visionBox.minX * imageWidth,
            y: (1.0 - visionBox.maxY) * imageHeight, // Yì¶• ë’¤ì§‘ê¸°
            width: visionBox.width * imageWidth,
            height: visionBox.height * imageHeight
        )
        
        print("ğŸ” Converted CGImage box: \(cgBox)")
        
        // ì–¼êµ´ ì˜ì—­ì„ 20% í™•ì¥ (ì•ˆì „í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í¬ë¡­)
        let expandRatio: CGFloat = 0.2
        let expandX = cgBox.width * expandRatio
        let expandY = cgBox.height * expandRatio
        
        let expandedBox = CGRect(
            x: max(0, cgBox.minX - expandX),
            y: max(0, cgBox.minY - expandY),
            width: min(imageWidth - max(0, cgBox.minX - expandX), cgBox.width + expandX * 2),
            height: min(imageHeight - max(0, cgBox.minY - expandY), cgBox.height + expandY * 2)
        )
        
        print("ğŸ” Expanded box: \(expandedBox)")
        
        // ê²½ê³„ ê²€ì‚¬
        guard expandedBox.width > 0 && expandedBox.height > 0 &&
              expandedBox.minX >= 0 && expandedBox.minY >= 0 &&
              expandedBox.maxX <= imageWidth && expandedBox.maxY <= imageHeight else {
            print("âŒ Invalid crop box dimensions or out of bounds")
            return nil
        }
        
        // ì´ë¯¸ì§€ í¬ë¡­
        guard let croppedCGImage = cgImage.cropping(to: expandedBox) else {
            print("âŒ Failed to crop CGImage")
            return nil
        }
        
        // í¬ë¡­ëœ ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ í™•ëŒ€
        let minSize: CGFloat = 200
        let croppedImage = UIImage(cgImage: croppedCGImage, scale: 1.0, orientation: .up)
        
        if max(croppedImage.size.width, croppedImage.size.height) < minSize {
            let scale = minSize / max(croppedImage.size.width, croppedImage.size.height)
            let newSize = CGSize(
                width: croppedImage.size.width * scale,
                height: croppedImage.size.height * scale
            )
            
            UIGraphicsBeginImageContextWithOptions(newSize, false, 1.0)
            defer { UIGraphicsEndImageContext() }
            
            croppedImage.draw(in: CGRect(origin: .zero, size: newSize))
            
            if let scaledImage = UIGraphicsGetImageFromCurrentImageContext() {
                print("ğŸ” Face image scaled up to: \(scaledImage.size)")
                return scaledImage
            }
        }
        
        print("ğŸ” Final cropped face size: \(croppedImage.size)")
        return croppedImage
    }
    
    func clearResults() {
        detectedFaces.removeAll()
        editableFaces.removeAll()
        error = nil
        isProcessing = false
        currentImageSize = .zero
    }
    
    // MARK: - ğŸ†• ì–¼êµ´ í¸ì§‘ ê¸°ëŠ¥
    
    /// ì–¼êµ´ ì¸ì‹ ê²°ê³¼ë¥¼ í¸ì§‘ ê°€ëŠ¥í•œ ì–¼êµ´ë¡œ ë³€í™˜
    func convertToEditableFaces(imageSize: CGSize) {
        currentImageSize = imageSize
        editableFaces = detectedFaces.map { face in
            EditableFace(from: face, imageSize: imageSize)
        }
        
        print("ğŸ“ Converted \(detectedFaces.count) detected faces to editable faces")
        print("ğŸ“ Image size: \(imageSize)")
    }
    
    /// ìƒˆë¡œìš´ ì–¼êµ´ ë°•ìŠ¤ ì¶”ê°€ (í–¥ìƒëœ ë²„ì „)
    func addNewFace() {
        guard currentImageSize != .zero else {
            print("âš ï¸ Cannot add face: image size not set")
            return
        }
        
        // ë” ë˜‘ë˜‘í•œ í¬ê¸° ê³„ì‚°
        let smartSize = calculateSmartBoxSize()
        let suggestedPosition = EditableFace.suggestPosition(
            for: smartSize,
            in: currentImageSize,
            avoiding: editableFaces
        )
        
        let newFace = EditableFace(
            boundingBox: CGRect(
                origin: suggestedPosition,
                size: smartSize
            ),
            confidence: 1.0,
            isUserAdded: true
        )
        
        editableFaces.append(newFace)
        
        print("â• Added new face box:")
        print("  â€¢ Position: \(suggestedPosition)")
        print("  â€¢ Size: \(smartSize)")
        print("  â€¢ Total faces: \(editableFaces.count)")
        
        // ì‹œê°ì  í”¼ë“œë°±ì„ ìœ„í•´ ì ì‹œ í•˜ì´ë¼ì´íŠ¸
        if let newIndex = editableFaces.firstIndex(where: { $0.id == newFace.id }) {
            editableFaces[newIndex].isHighlighted = true
            
            // 2ì´ˆ í›„ í•˜ì´ë¼ì´íŠ¸ í•´ì œ
            DispatchQueue.main.asyncAfter(deadline: .now() + 2.0) {
                if newIndex < self.editableFaces.count {
                    self.editableFaces[newIndex].isHighlighted = false
                }
            }
        }
    }
    
    /// ë˜‘ë˜‘í•œ ë°•ìŠ¤ í¬ê¸° ê³„ì‚°
    private func calculateSmartBoxSize() -> CGSize {
        if editableFaces.isEmpty {
            // ì²« ë²ˆì§¸ ë°•ìŠ¤ì¸ ê²½ìš° ì´ë¯¸ì§€ í¬ê¸°ì— ë¹„ë¡€í•œ ê¸°ë³¸ í¬ê¸°
            let defaultRatio: CGFloat = 0.15 // ì´ë¯¸ì§€ì˜ 15%
            let size = min(currentImageSize.width, currentImageSize.height) * defaultRatio
            return CGSize(width: size, height: size * 1.2) // ì•½ê°„ ì„¸ë¡œë¡œ ê¸´ í˜•íƒœ
        }
        
        // ê¸°ì¡´ ë°•ìŠ¤ë“¤ì˜ í‰ê·  í¬ê¸° ê³„ì‚°
        let averageSize = EditableFace.averageSize(from: editableFaces)
        
        // í¬ê¸° ë²”ìœ„ ì œí•œ (ë„ˆë¬´ ì‘ê±°ë‚˜ í¬ì§€ ì•Šë„ë¡)
        let minSize: CGFloat = 60
        let maxSize = min(currentImageSize.width, currentImageSize.height) * 0.3
        
        let clampedWidth = max(minSize, min(maxSize, averageSize.width))
        let clampedHeight = max(minSize, min(maxSize, averageSize.height))
        
        return CGSize(width: clampedWidth, height: clampedHeight)
    }
    
    /// ì–¼êµ´ ë°•ìŠ¤ ì‚­ì œ
    func removeFace(withId id: UUID) {
        guard editableFaces.count > 1 else {
            print("âš ï¸ Cannot remove face: minimum 1 face required")
            return
        }
        
        if let index = editableFaces.firstIndex(where: { $0.id == id }) {
            let removedFace = editableFaces.remove(at: index)
            print("âŒ Removed face: userAdded=\(removedFace.isUserAdded)")
            print("ğŸ“Š Total faces: \(editableFaces.count)")
        }
    }
    
    /// ì–¼êµ´ ë°•ìŠ¤ ìœ„ì¹˜ ì—…ë°ì´íŠ¸ (í–¥ìƒëœ ë²„ì „)
    func updateFacePosition(id: UUID, dragOffset: CGSize) {
        if let index = editableFaces.firstIndex(where: { $0.id == id }) {
            editableFaces[index].dragOffset = dragOffset
            editableFaces[index].isDragging = true
        }
    }
    
    /// ë“œë˜ê·¸ ì™„ë£Œ ì‹œ ìœ„ì¹˜ ì ìš© (í–¥ìƒëœ ë²„ì „)
    func finalizeFacePosition(id: UUID) {
        if let index = editableFaces.firstIndex(where: { $0.id == id }) {
            // ê¸°ì¡´ ë¡œì§ ìœ ì§€í•˜ë©´ì„œ ì¶”ê°€ ê²€ì¦
            editableFaces[index].applyDragOffset()
            editableFaces[index].constrainToImage(size: currentImageSize)
            
            let finalBox = editableFaces[index].boundingBox
            print("ğŸ“ Finalized face position:")
            print("  â€¢ Box: \(finalBox)")
            print("  â€¢ Image bounds: \(currentImageSize)")
            print("  â€¢ Is within bounds: \(isWithinBounds(finalBox))")
        }
    }
    
    /// ë°•ìŠ¤ê°€ ì´ë¯¸ì§€ ê²½ê³„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
    private func isWithinBounds(_ box: CGRect) -> Bool {
        let imageBounds = CGRect(origin: .zero, size: currentImageSize)
        return imageBounds.contains(box)
    }
    
    /// í¸ì§‘ëœ ì–¼êµ´ë“¤ì„ DetectedFace í˜•íƒœë¡œ ë³€í™˜ (ë£°ë ›ìš©)
    func getEditedFacesAsDetected() -> [DetectedFace] {
        return editableFaces.map { editableFace in
            // í”½ì…€ ì¢Œí‘œë¥¼ Vision ì¢Œí‘œë¡œ ì—­ë³€í™˜
            let visionBox = CGRect(
                x: editableFace.boundingBox.minX / currentImageSize.width,
                y: 1.0 - (editableFace.boundingBox.maxY / currentImageSize.height),
                width: editableFace.boundingBox.width / currentImageSize.width,
                height: editableFace.boundingBox.height / currentImageSize.height
            )
            
            var detectedFace = DetectedFace(
                boundingBox: visionBox,
                confidence: editableFace.confidence
            )
            
            // ê¸°ì¡´ í¬ë¡­ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë‚˜ì¤‘ì— ìƒì„±
            detectedFace.croppedImage = editableFace.croppedImage
            
            return detectedFace
        }
    }
}
